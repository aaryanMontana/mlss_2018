\chapter{Deep Learning by \href{https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php}{Rob Fergus}}
%\newrefsection

 14:30--16:00

``Rob Fergus is an Associate Professor of Computer Science at the Courant
Institute of Mathematical Sciences, New York University. He is also a Research
Scientist at Facebook, working in their AI Research Group. He received a
Masters in Electrical Engineering with Prof. Pietro Perona at Caltech, before
completing a PhD with Prof. Andrew Zisserman at the University of Oxford in
2005. Before coming to NYU, he spent two years as a post-doc in the Computer
Science and Artificial Intelligence Lab (CSAIL) at MIT, working with Prof.
William Freeman. He has received several awards including a CVPR best paper
prize, a Sloan Fellowship \& NSF Career award and the IEEE Longuet-Higgins
prize.'' -- \href{https://cs.nyu.edu/~fergus/pmwiki/pmwiki.php?n=PmWiki.Bio}{Personal
Page from New York University}

\section{Deep Supervised Learning}

\subsection{History of Neural Nets}

The connectionsims started around 40's with Hebb's work, McCulloch and Pitts
and Rosenblatt 50's.

Second era started around 80's with Backpropagation to train multi-layered
networks with the work of Rumelhart, Hinton and Williams. And the architecture
of the Convolutional neural networks by Yann LeCun.

The era of Deep learning starts around 2011 with tasks focused on vision,
speech understanding and natrual language processing. The main ingredients
were (1) supervised training of deep networks with (2) faster GPUs and (3) big
labeled datasets.


\subsection{Deep learning vs traditional approaches}

The traditional approach consists on the hand-designed feature extraction that
may be used by a simple classifier in order to predict the labels of a set of
data. On the oposite hand, deep neural networks are focused on learning useful
feature representations that improve the performance of the model on the
dataset.

From 2010 to 2015 there is a clear decrease on the error rate in the ImageNet
tasks by ussing more complex neural networks. The number of layers goes from 8
layers with the AlexNet in 2012 to 152 layers with the ResNet in 2016.

There are similar jumps in the performance of speech recognition systems. As an
example, in 2015 Baidu0's Deep Speach 2 system used 100 million parameteres in
11 layers of a Recurrent Neural Network. It was trained in around 11.940 hours
of English.

In the task of natural language processing and Machine Translation the work by
Sutskever et al. and Cho et.al 2014. Tests of perplexity show a linear decrease
from 2013?

One of the benefits of Deep learning is that altought there is a huge
computational cost during training, it is really lightweight during deployment.
This means that small devices like smartphones are able to perform real-time
predictions. One example is the detection of cars, road and pedestrians on
self-driving cars.

Other interesting areas where deep learning is being applied are: astronomy,
healthcare (e.g. skin cancer classification, or diabetic retinopathy).

\subsection{Some issues with Deep Learning}

(1) There is a missing theoretical understanding and performance guarantees.
(2) Difficult to inspect the models.
(3) Need lots of labeled data
(4) We are still hand-designing the network architecture (instead of the features).
There are some attempts to learn automatically the architecture (meta-learning)
like Neural architecture search with reinforcement learning.
(5) The most generic architectures (i.e. fully connected) does not seem to
work, and it seems that their architecture is really dependent on the domain
(e.g. for images exploit 2D)

\subsection{Convolutional Neural Networks}

First designed in the work of Yann LeCun (and previously Fukushima) by trying to
mimic the visual system (of cats in Fukushima). It tries to exploit the spatial
relation of pixels. In the work of Fukushima there was no back-propagation and
the architecture could not be learned automatically. It was after
back-propagation was applied into neural networks in 1986 that Yann LeCunn
could show a practical application by classifiying handwritten digits from a
dataset generated by some american post offices with Yann LeCunn as an
investigator (TODO rephrase previous sentence).

If the traditional activation functions were sigmoid shaped like the hyperbolic
tangent or the logistic function, in more recent years activations with linear
regions started to become more popular (e.g. ReLu).

Anothre useful trick is the Batch Normalisation, that has been empirically
proved to improve the performance of CNNs. This method consists on normalizing
the input features with 0 mean and 1 standard deviation on every mini-batch.
There are two extra parameters (TODO missed theses parameters).

\subsection{Stochastic Gradient Descent}

This is an iterative learning method that usually trains with mini-batches.

\begin{equation}
  \text{missing}
\end{equation}

Another training method is the AdaGrad

\begin{equation}
  \theta_{t+1} = \theta_t - \text{missing}
\end{equation}

RMSProp

\begin{equation}
  \text{missing}
\end{equation}

and ADAM

\begin{equation}
  \text{missing}
\end{equation}

\subsection{Some practical debugging tips from M. Ranzato}

\begin{itemize}
  \item Train on small subset of data: the training erro should go to zero
  \item Training diverges: learnoing rate may be too large (decrease learning
    rate), or the back-propagation is buggy because of numerical gradient
    issues.
  \item The parameters collapse, the loss is minimized but the training
    accurayc does not increase: check the loss function
  \item TODO some other tricks missing
\end{itemize}

\subsection{Weights' initialization}

With a simple example with a linear activation with 1 layer neural network we
have

\begin{equation}
  Var[y] = (n^{in}Var[w])Var[x]
\end{equation}

While in a multilayer network

\begin{equation}
  Var[y] = (\prod_d n_d^{in}Var[w_d])Var[x]
\end{equation}

This value during the forward propagation will tend to explode with the depth
$d$. And the backward propagation will make the gradients vanish.

\begin{equation}
  \text{missing derivative of previous equation}
\end{equation}

This makes the initialization really important for a good convergence of the
weights into a good region within reasonable number of epochs (shows empirical
comparison with good and bad initializations).

\subsection{Deep Residual Learning}

It allows some layers to bypass the next layer. This allows the network to skip
layers if these are not necessary for the precition. Empirical experiments show
a decrease from 7.4 error rate (with 34 layers) to 5.7 (with 152 layers).

Some additional information is described in ``Tradeoffs of Large Sclae
Learning, Bottou \& Bousquet, 2011''.


\subsection{Weakly supervised pretraining}

Some recent work on using weakly labeled images from the internet in order to
pretrain deep neural networks.

\href{https://arxiv.org/abs/1511.02251}{Learning Visual Features from Large
Weakly Supervised Data} \cite{Joulin2015}

\section{Unsupervised Learning}

Learning without labels the intrinsic structure of the data. This is
practically important as the real-world categories follow a Zipf's law, meaning
that lots of categories appear very few times.

Some of the benefitrs of unsupervised learning are that there is a vast amount
of free data available, and it is potentially useful as a regularisation
method.

The basic idea is to be able to build a model of $p(X)$ (density modeling)
given just data $\{X\}$.

\subsection{Self supervised learning}

This method consists on using the input data as some part of the target data

\begin{align}
  y: X \rightarrow Y \\
  x \rightarrow y(x)
\end{align}

and use the same techniques used in supervised learning to train a model


\begin{equation}
  \argmin_\theta \frac{1}{n} \sum_{i=1}^n l(f_\theta(x_i), y(x_i))
\end{equation}

This approach usually requires some knowledge of the domain. Some examples are
word2vec in wich the word in the central position of a sentence is considered
the target prediction.

\subsection{Auto-Encoder}

This method tries to find a hidden representation that keeps as much
information of the input data as possible by reducing the reconstruction error.
The network is divided between a decoder and a encoder part.

\subsection{Variational Auto-Encoder}

Similar  to the simple autoencoder but makes


\subsection{Generative Adversarial Networks}

This consists in a decoder-only model but that uses an advesarial loss term
using a discriminator $D$. Mini-max game between G and D.

\subsection{Stacked Auto-Encoders}

The Ladder Networks (Rasmus et al. 2015) adds a reconstruction constrain at
every layer. In this way, there is a loss between every layer that can be
minimized. This is like being able to generate hidden variable states apart
from the input data.

\subsection{Other approaches}

Autoencoders, restricted / Deep Boltzmann Machines, \dots

\subsection{Application examples}

Stochastic video generation (Denton and Fergus 2018) tries to predict the
following frames from a video using encoders, decoders and Long Short Term
Memories. The underlaying method is a Variational Auto-Encoder.

An application example is shown with synthetically generated videos using MNIST
dataset and two bouncing numbers in a closed square. The task of the model is
to predict the position of the numbers in the future (pixel by pixel?). It is
possible to sample from the learned distribution, possibly generating an
un/certainty heat-map.

Training a generative model to add color to images from just their luminance
levels.

Training a generative model to predict the word in the middle of a sentence.

Cut an image into different sections (3x3 squares?) and train a model to
predict to which part of the image every pice belongs to. Unsupervised Learning
of Visual Representations by Solving Jigsaw Puzzles \cite{Noroozi2016}.

Automatic learning of images and audio from videos \cite{Owens2016}. This
method shows how it is possible to know the source of a sound from an image,
some examples show people talking and rainfalls.

Map every image into a hypersphere \cite{bojanowski2017}, hopefully similar
images will fall into nearby regions?

Unsupervised learning of visual representations using videos
\cite{wang2015unsupervised}

\section{Deep Learning Models Rob Fergus 9:30--11:00}

Most of the deep learning models are exploiting some structural bias of the
data. For example, images have a 2D correlation pixelwise, while speech
recognition and natural language processing exploits a 1D time correlation.

\subsection{Memory in Deep networks}

Deep neural networks are not able to store memory to solve sequential problems.
Some networks are able to store implicit internal memory on their connections
like Recurrent Neural Networks and Long Short Term Memory networks.

\subsubsection{Implicit internal memory}

Recurrent neural networks have the computational and the memory integrated in
their weights. However, one of the main problems of RNNs is how to prevent the
network from forgetting during the training. In order to solve that problem
Mikolov et al. 2014 separated the state into a fast and slow changing part.
Other approaches are by using gated units for the internal state like
Long-short term memory (LSTM) (see Hochreiter \& Schmidhuber 1997, Graves,
2013), or Simplified minimal gated unit variations for RNNs. Also GRU
light-weight version from Cho et al 2014.

RNN search: attention in machine translation \cite{bahdanau2014neural}
investigates an encoder and decoder model in a RNN, similar method was used
later for image caption generation with attention \cite{xu2015show}, this last
method allows the network to be queried and create heatmaps on the original
images to correlate the generated words with their corresponding patch.

\subsubsection{External Global Memory}

Some work tries to split the computational memory from the problem solving
memory aspect. These two parts are called the \emph{memory module} and the
\emph{controller module}. The controller is able to write and read to the
memory module. The memory needs some addressing mechanism, and can be soft or
hard. The benefit of soft addressing is that it can be trained by
backpropagation, while the hard addressing is not differentiable.

The end-to-end memory network (MemN2N) \cite{SukhbaatarSWF15} is an
architecture. The memory module is able to store some input vectors and hidden
states, the controller module is able to search in the memory for similar
patterns and obtain the corresponding associated output. This can be a new
hidden state that will be combined with the current hidden state of the
controler.

\begin{enumerate}
  \item Embed each word (word to vector?)
  \item Sum embedding vectors ($v_{Sam} + v_{drops} + v_{apple} = m_i$)
  \item It generates one memory per sentence
  \item The controller takes the vector encoding of the question and makes a
    dots product with the memory sentences and applies a softmax.
  \item This guives weights to each sentence it computes a weighted sum of
      the sentences' representations.
  \item The representation is guiven to the controller and generate an answer
    from it.
\end{enumerate}

Another example is the Neural Turing Machine \cite{graves2014neural} in which
the authors design an end-to-end diferentiable architecture that may be trained
by backpropagation to solve tasks involving memory. Some example applications
are coping, sorting and associative recall of inputs with outputs.

\section{Deep Nets for sets}

There ar problems where there is permuation invariance, dynamic sizong, single
output, or output for each element.

In the Communication Neural Network (CommNet) \cite{sukhbaatar2016} the inputs
and outputs are sets, another example is the DeepSets \cite{zaheer2017deep}.
The CommNet is a special case of a Graph NN in which a set can be represented
as a complete graph.


