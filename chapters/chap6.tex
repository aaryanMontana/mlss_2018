\chapter{Supervised Learning and Text Classification by
\href{http://www.kyunghyuncho.me/}{Kyunghyun Cho}}

\section{Introduction to supervised learning with ANNs Wed.  11:30--13:00}

An overview on supervised learning, in which the input is described as a
validation set $D_{val}$ and a test set $D_{test}$

At the end we need to choose the hypothesis that best adjusts to our problem
between the availables. Examples of hyperparameters sets are in SVMs the
regularization parameters and the kernel, simiarly with Gaussian Processes with
the kernel and the parameters $\sigma^2$ and \emph{lenght-scale}.

In Artificial Neural Networks the hypothesis set is the set of architectures,
and hyperparameters. The architecture is commonly an directed acyclic graph
(DAG) with parameters, inputs, outputs and compute nodes (functions that are
often differentiable).

An example of architecture is a logistic regression where

\begin{equation}
  p_\theta(y=1|x) = \sigma(w^T x + b = \frac{1}{1 + \exp(-w^T x - b)}
\end{equation}

or a 3rd-order polynomial function.

The inference is done by forward propagation of the input to the output trhough
all the hidden layers.

Supervised learning tries to find a function $f_\theta(x)$, while in the case
of Neural Networks we can usually interpret it as computeing the conditional
probabilities guiven the input $p(y='|x)$. This is achieved by computing any
arbitrary output values from a network, then exponentiating every individual
prefdiction and dividing them by their sum (soft-max function).

The objective is that the training data is maximally maximized, by ensuring
that the maximum individual probabilities is maximized at the same time.

\begin{equation}
  \argmax_\theta \log p_\theta(D) = \argmax_\theta \sum_{n=1}^N \log
  p_\theta(y_n|x_n)
\end{equation}

We can also use the log-likelihood

\begin{equation}
  \text{Missing equation}
\end{equation}

\subsection{Loss minimization}

In order to minimize the loss it is necessaGraph Neural Networkry to use optimization techniques,
in the case of \glspl{ANN} is by using backpropagation to
compute the partial derivatives of the parameters with respect the input using
the chain rule of derivatives

\begin{equation}
  \frac{\partial(f o g)}{\partial x} = \frac{\partial f}{\partial
  g}\frac{\partial g}{\partial x}
\end{equation}

This differentiation is done automatically by
\href{https://github.com/HIPS/autograd}{autograd} which will implement the
Jacobian-vector product of each P node.

By doing backpropagation we obtain the gradient of the loss with respect to
every parameter $\theta$. Instead of computing the gradient for the full
dataset, it is common to use mini-batches with a method called Stochastic
Gradient Descent.

\begin{enumerate}
  \item Random subset of M training examples
  \item Compute the minibatch gradient
    \begin{equation}
      \Gradient L \sim 1/N' \sum_{n=1}^{N'} \text{missing part}
    \end{equation}
  \item Update the paraemters $\text{missing equation}$
  \item repeat until the validation loss stops improving
\end{enumerate}

However, this method will find a local minima in the training set, in order not
to overfit to the training data, a validation set is used to do an early stop.
This is one of the most important parts of the training as we want the minima
to be as near as possible to the dataset near.

\section{Text classification}

In text classificatio the input are sentences or paragraphs and the output is a
category to which the input belongs to (commonly a fixed number of $C$
categories).

Some of the particularities of using sentences as imputs is that they are of
variable size $X = (x_1, x_2, \dots , x_T)$ where every $x_t$ is a token from
a vocabulary $V$. This is done by automatically (or manually) splitting all the
individual words and creating an index of words that will form our vocabulary
$V$. Then every sentence is encoded by a sequence of integers. In this case we
are not tokenizing the words first by extracting any meaning of the word, in
this sense the words ``cat'' and ``cats'' have completly independent indices.

At the end the words are encoded as one-hot-encoding with a bit to 1 for the
corresponding index. This will be given to the \gls{ANN} and will be fordard
propagated to obtain a hidden representation $e_i$. This can be interpreted as
a table lookup from a word to a hidden embedding (eg. a fixed matrix $W$ of
  dimmension $\text{\#tokens} \times \text{\#arbitrary dimesion}$ that is
multiplied by the binary token).

The representation of a sentence is a continuos bag-of-words, this means that
we ignore the order of the words in the sentence, and average the corresponding
token vectors. The method has been proven to be very useful in the works
Iyyer2016, cho2017, and in FastText by Bojanowski2017.

\subsection{How to represent a sentence}

When the order of the words is important, it is possible to encode every set of
words. For example, Relation Network: skip bigrams consider all the possible
pairs of tokens and averages all the relationship vectors. The relations
between words can be encoded depending on the problem, we could consider that
the order of the bigrams is not important, only interested on bigrams of
contiguous words, all the possible pairs, or skip some words.

With the previous idea is possible to use Convolutional Neural Networks in
order to convolve the the ``look up table'' trough the input
\cite{kim2014convolutional}.

The CNN can also represent the bigram representations, and it is possible to
learn a weight vector that will choose how many words are important. Look at
the relation between Relational Networks and Convolutional Neural Networks.

\subsubsection{Self-attention 11:30--13:00}

Self-attention is a generalization of a CNN and RN that is able to capture
long-range dependecies with a single layer. It can also ignore irrelevant
long-term dependencies. Also mention generalization with multi-head and
multi-hop attention.

Using a Recurrent Neural Network we can create an online representation of a
sentence by reading every word and storing their representation in the
recursive hidden representation. This allows a cost of $O(T)$ instead of the
$O(T^2)$ necessary to do all the word pairs.

The representation that is generated by the RNN can encode the representation
of a region of the text given it's context. This can then be feeded to the
previously seen atention model that can learn the weighted sum of the context.

In ``Fully character-level neural machine translation without explicit
segmentation'' \cite{lee2016fully} the authors stack a RNN on top of CNNs.

\section{Natural Language Models}

\subsection{Autoregressive language modelling}

The autoregressive sequence modelling assumes that the past tokens influence
the current token as

\begin{equation}
  p(X) = p(x_1)p(x_2|x_1)\cdots p(x_T|x_1,\dots,x_{T-1})
\end{equation}

this holds true given the conditional distribution assumption.

With this method, an unsupervised method is transormed in $T$ smaller
supervised problems.

One think to keep in mind from a question that was asked is that altough the
marginalisation of $p(X)$ should sum to one, in real cases with RNNs this is
not always true. Possibly because of the parametrization.

The autoregressive sequence modelling can be represented as

\begin{equation}
  p(X) = \prod_{t=1}^T p(x_t|x_{<t})
\end{equation}

In order to score a sentence we can compare the output of the Autoregressive
model for several sentences and use softmax to obtain ``probabilities'' (values
between 0 and 1 that sum to one) for each sentence.

\subsection{N-Gram language models}

Before \glspl{ANN} were used, the idea about ussing the conditional probabilities was
already used on a smaller scale with N-gram models. In this case the $N$ needs
to be decided in advance.

\begin{align}
  p(x|x_{-N},x_{-N+1},\dots,x_{-1}) = \frac{p(x_{-N},x_{-N+1},\dots,x_{-1}, x)} {\sum_{x \in V}p(x_{-N},x_{-N+1},\dots,x_{-1}, x)}
\end{align}

The process then is to get the dataset and count the frequencies of every
occurrence of the tokens $\dots,x_{N-1}$ followed by all the possible tokens
$x$.

There are two main issues that araise by ussing frequentist N-Grams

\begin{enumerate}
  \item Data sparsity: lack of generalization
    \begin{itemize}
      \item If using a 3-gram model you find 3 words that never happened again,
        the product of probabilities will be zero independently on the rest.
      \item One possible solution is to use smoothing by adding a small
        constant
      \item Another solution is to try with all $n \in \{N,\dots,1\}$
    \end{itemize}
  \item Inability to capture long-term depenfdencies
\begin{itemize}
  \item By choosing a fixed $N$ we may lose long term dependencies.
\end{itemize}
\end{enumerate}

\subsection{Neural N-Gram Language Model}

\cite{bengio2003neural} solved some of the previous issues by using the
hidden representation of a Neural Network instead of the tokens. In the
continuous vector space the similar tokens or phrases are nearby.

Some other work on the same direction is \cite{mikolov2013distributed},
, \cite{pennington2014glove}, \cite{le2014distributed}.

An example of this application is the generalization of sentences that never
happened by realizing that some words share some similarities (eg. numbers). An
example was shown in which sentences with the 2-grams ``three teams'', ``four
teams'' and ``four groups'' are able to generalise to the bigram ``three
groups'' by realizing that three and four share a similar continuous space, and
that before groups there could be a number.

In practice

\begin{enumerate}
  \item Collect TODO missing steps
\end{enumerate}

\subsection{Convolutional Language Models}

Convolutional Neural Networks allow to extend the context size by applying the
convolution through larger parts of the text (see kalchbrenner et al, 2015 and
Dauphin et al 2016, ByteNet by \cite{kalchbrenner2016neural}, PixelCNN,
WaveNet, \dots)

Gated convolutional language model by Dauphin 2016 \cite{dauphin2016language}


\subsection{CBoW Language Models (infinite context)}

The idea is similar to the LM of using averages instead of concatenation.

\subsection{Recurrent Language Models}

An RNN can summarize all the tokens seen until $x$ into a continuous vector
representation Mikolov et al, 2010 \cite{mikolov2010recurrent}.

\subsection{Recurrent Memory Networks}

The work of Tran et al., 2016 \cite{tran2016recurrent} combines RNNs to compress the context into a
continuous vector representation with the attention model that learns the
weighting of the context.

\section{Recurrent Networks and Backpropagation}

Consider the full path from a parameter $\theta$ to the loss $l$. The
backpropagation consists on computing the gradient of the $l$ with respect to
the previous node and multiply by the Jacobian matrix of every step back to the
parameter $\theta$.

\begin{equation}
  \Jac_{h^{t+1}}^{h^t} = W^T \text{diag}(\tanh'(a^t))
\end{equation}

Because the Jaciobians are multiplied by every backpropagation step, this means
that

\begin{itemize}
  \item If $W > 1$ (the upperbound of ) the norm blows up, exploding gradient
  \item If $W < 1$ (the upperbound of) the norm shrinks to zero, vanishing
    gradient
\end{itemize}

\begin{equation}
  || \prod_{t'=t}^{} || \le  || \Jac || \text{TODO missing part}
\end{equation}

\subsection{Gated recurrent units}

These type of networks allow to skip some of the paths in order to avoid the
exploding or vanishing gradient.

\begin{itemize}
  \item Adaptive shortcut:
  \item Candidate update + pruning
  \item Update gate: $u_t = \sigma(W_uh_{t-1} + U \cdots$
  \item reset gate $r_t = \sigma(W_rh_{t-1} + U_rx + b_r)$
\end{itemize}

\subsection{Lessons from GRU/LSTM}

\begin{itemize}
  \item Credit assignment over a long path of computation is difficult
    \item Adaptive shorcut or skip-connection helps avoid credit dilution
      \item Gates are an effective way to focus credit assignment
\end{itemize}

\section{Neural Machine Translation 14:30--16:00}

\subsection{History of machine translation}

The original idea was to get a text from one language, (1) perform a
morphological analysis, (2) a syntactic analysis, (3) semantic analsysis, (4) a
semantic composition and obtein an interlingua text that can be transformed
back to any other language Borr, Hovy and Levin 2006 \textbf{TODO missing
citation}.

Allen 1987 ieee icnn, a brief resurrection of Neural Networks in 1997 by
Castano and Casacuberta 1997 \cite{castano1997machine}, then in 2006 Schwenk
2006 \cite{schwenk2006continuous} as a filter source to SMT to \gls{ANN} to target sentence, then Devlin 2014 from
source to SMT + \gls{ANN} to target sentence, then source to
\gls{ANN} to target sentence.

\subsection{Encoding: Token representation}

First it is necessary to build a source and target vocabulary of unique tokens
(for each language). Then transform the text into the set of tokens. Then
encode the token sentences into sentence representation vectors, being careful
not to compress the sentences into small vectors that may lose useful
information.


\subsection{Decoding: conditional language modeling}

Using autoregressive networks we are interested in predicting the posterior
probability

\begin{equation}
  p(Y|X) = \prod_{t=1}^T p(y_t|y_{t-1}, X)
\end{equation}

Look at the RNN Neural Machine Translation by Bahdanau et al., 2015
\cite{bahdanau2014neural}. The model uses the target sentence (and what has
been translated until the current moment) in order to generate the following
prediction.

\begin{enumerate}
  \item Encode: read the entire source sentence to know what to translate
  \item Attention:
  \item Decode:
  \item Repeat 2 to 3 until the end-of-sentence (token) is achieved
\end{enumerate}

This method achieved performance as good as the current state-of-the-art
alternative at the moment phrase-based machine translation (PBMT).

At translation time every predicted word of the sentence had an associated
vector of weights that indicted the source words involved, altough the model
was trained only with pairs of text without any extra supervision. We should
consider that every token of the source sentence is at the same time associated
with a context in its language (see \cite{jean2015montreal}).

\subsection{In practice}

Available frameworks

\begin{itemize}
  \item Nematus \cite{sennrich2017nematus}
  \item OpenNMT-py \cite{opennmt}
  \item FairSeq \cite{gehring2017convs2s}
  \item Sockeye \cite{Sockeye:17}
\end{itemize}

\section{Current and ongoing projects}

Multilingual translation, real-time translation, and character level
translation.

\subsection{Multilingual translation}

A common approach has been to use a pivot language to translate languages
without lots of examples. For example, translate korean to Japanese and then to
English as the corpus between them is larger than the Korean to English.

With this idea, there has been some work to generate a pivot common language
that is automatically learned in an \gls{ANN}. For example, an encoder/decoder
aproach in \cite{firat2016multi, firat2016zero} creates one encoder per source
and decoder per target, and a model is learned for every pair. Later Johnson et
al, 2016, Ha et al, 2016, Lee et al 2017, and Gu et al, 2018 work on an
Universal \dots

A current limitation is these methods drastically depend on the different
amount of available data in each language. The models start ignoring the less
frequent language. However, if given the same proportions of samples for every
language they may overfit to the less frequent one because of the multiple epochs
run on one language compared to the other.

Some work to solve this problem is being done with Meta-Learning MAML in
\cite{finn2017model}. The difference with multitask learning is that  \dots

Another idea is to create the unsupervised lookup tables to convert word tokens
into continuous vectors ussing big corpus of different languages. In this
manner, words with simmilar meanings will fall into similar regions (see
\cite{artetxe2017unsupervised}). In this case, the overfiting is less probable
to happen as the lookup table is only learning a mapping of tokens to a
continuoius space, but not the translation. Then, the translation model is
trained on top of it.

\subsection{Real-Time Translation (learning to decode)}

learning to translate in real-time with neural machine translation
\cite{gu2016learning}

In order to generate the best translation it is possible to generate several
and then select the most probable one. However, ocasionally it may happen that
after several words are translated the topic changes and the best translation
may be a different one. In order to solve this the Beam Search keeps track of
parallel best translations and is able to switch to another one at any point if
necessary.

\begin{mybox}
\textbf{Exploiting the hidden activation}

In a Deep Neural Network huge information in the hidden layers is usually
discarded in order to predict a class; obtaining at the end a binary
prediction. However, the hidden information has rich information about the
original input that can be exploited.
\end{mybox}

By using the hidden information the performance of several methods was
increased \cite{gu2016learning}.
